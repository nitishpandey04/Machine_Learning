{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQdvS8UUV8L7"
      },
      "source": [
        "# Oil-Storage Tank Instance Segmentation with Mask R-CNN \n",
        "------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFYoaKUTWKam"
      },
      "source": [
        "*by Georgios K. Ouzounis*\n",
        "\n",
        "In this notebook we will experiment with Mask R-CNN model-training on custom data using transfer learning and try instance segmentation with our new model. The goal is segment oil-storage tanks from very high resoltion satellite images.\n",
        "\n",
        "**Code**\n",
        "\n",
        "The code in this notebook makes use of material gathered and modified from several repos listed below:\n",
        "- [Mask R-CNN](https://github.com/matterport/Mask_RCNN) by [Matterport, Inc](https://matterport.com/),\n",
        "- [Mask R-CNN extended for TensorFlow2.x](https://github.com/ahmedfgad/Mask-RCNN-TF2) by [Ahmed Gad](https://github.com/ahmedfgad),\n",
        "- [Image to COCO JSON converter](https://github.com/chrise96/image-to-coco-json-converter), by [chrise96](https://github.com/chrise96).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbyRQKHGua0v"
      },
      "source": [
        "<img src=\"https://github.com/georgiosouzounis/instance-segmentation-mask-rcnn/raw/main/images/banner.jpg\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAGw8-zougmB"
      },
      "source": [
        "**Data**\n",
        "\n",
        "The dataset used is posted in [Kaggle](www.kaggle.com) by [Airbus](https://www.airbus.com/) Defense and Space Intelligence and is titled [Airbus Oil Storage Detection Dataset](https://www.kaggle.com/airbusgeo/airbus-oil-storage-detection-dataset). It is a sample dataset of a larger one uploaded in Kaggle for a competition. \n",
        "\n",
        "It consists of a number of satellite images (RGB bands only) showing oil-storage tanks in various geographies. The annotation data is given in a CSV file containing the bounding boxes of all tanks that are visually distinctive.\n",
        "\n",
        "Please review the [License here](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode) and the [Disclaimer here](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeNJ8phsx3nu"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. [Install libraries](#install-libraries)\n",
        "2. [Get the image dataset](#get-the-image-dataset)\n",
        "3. [Prepare the dataset](#prepare-the-dataset)\n",
        "4. [Get the code](#get-the-code)\n",
        "5. [Get the COCO weights](#get-the-coco-weights)\n",
        "6. [Model Training](#model-training)\n",
        "7. [Model Inference](#model-inference)\n",
        "8. [Conclusions](#conclusions)\n",
        "--------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv6FJrrrw3Vy"
      },
      "source": [
        "## Install libraries\n",
        "\n",
        "**IMPORTANT:** Change your Runtime to GPU before starting with this notebook\n",
        "\n",
        "**Package compatibility - dependency conflicts**\n",
        "\n",
        "Due to a number of conflicting dependencies between the selected packages, we first need to unistall the latest versions of Keras and Tensorflow and in place install older versions. This project was tested successfully using:\n",
        "- tensorflow 2.2.0\n",
        "- keras 2.3.1\n",
        "- h5py 2.10\n",
        "\n",
        "**NOTE:** there is no need to restart the Runtime after installation. If the notebook promts you to do so, there is something wrong; close it and re-open it as it may have cached items that trigger conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmtIBLzpvNDl"
      },
      "outputs": [],
      "source": [
        "!pip uninstall keras -y\n",
        "!pip uninstall keras-nightly -y\n",
        "!pip uninstall keras-Preprocessing -y\n",
        "!pip uninstall keras-vis -y\n",
        "!pip uninstall tensorflow -y\n",
        "\n",
        "!pip install tensorflow==2.2.0\n",
        "!pip install keras==2.3.1\n",
        "!pip install h5py==2.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DO5kCuBW1oI"
      },
      "source": [
        "the above takes approximately 2.5 mins to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKoOAlkpH-2b"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkW5Skz1CWbT"
      },
      "source": [
        "--------\n",
        "## Get the image dataset <a name=\"get-the-image-dataset\"></a>\n",
        "\n",
        "In this section we will work with a sample image dataset of [oil-storage tanks](https://www.kaggle.com/airbusgeo/airbus-oil-storage-detection-dataset) obtained from [kaggle.com](www.kaggle.com). The dataset is a kind contribution of Airbus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8n8CIjmufFw"
      },
      "source": [
        "### Connect to Kaggle.com\n",
        "\n",
        "We will import this dataset into our working session directly from Kaggle. To read more on how to import kaggle datasets directly into Google Colab please refer to this [documentation page](https://www.kaggle.com/general/74235)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRl583VrulhB"
      },
      "outputs": [],
      "source": [
        "# install the kaggle API\n",
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNzBKSG0umA9"
      },
      "outputs": [],
      "source": [
        "# create a kaggle directory\n",
        "!mkdir ~/.kaggle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MPTQz5VuoeZ"
      },
      "source": [
        "Right-click on the side bar and select Upload File in the root directory. Choose your kaggle.json token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go7uKD7gutrx"
      },
      "outputs": [],
      "source": [
        "# move the token into the kaggle directory\n",
        "!mv /content/kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxLiobOruw4S"
      },
      "outputs": [],
      "source": [
        "# change the permisions of the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxUgmYrSu2He"
      },
      "outputs": [],
      "source": [
        " # test it out\n",
        " !kaggle datasets list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL6vdSZPu4Q0"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN8-qnrTvAke",
        "outputId": "9a6c6341-dc3c-4706-d295-d363802158fd"
      },
      "outputs": [],
      "source": [
        "# start the download\n",
        "%cd /content/\n",
        "%mkdir dataset\n",
        "%cd dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ne_BMbPCFK",
        "outputId": "8ca8a805-4290-4837-d228-71edc51450d9"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download airbusgeo/airbus-oil-storage-detection-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsAM-nTGvDe0"
      },
      "source": [
        "the above takes approximately 20 sec to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of2A5GSVMoxH"
      },
      "outputs": [],
      "source": [
        "!unzip airbus-oil-storage-detection-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk2NI3RuM027"
      },
      "outputs": [],
      "source": [
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYPWRrCNJu6C"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/content/dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbAoVyEoXCoW"
      },
      "source": [
        "--------\n",
        "## Prepare the dataset <a name=\"prepare-the-dataset\"></a>\n",
        "\n",
        "The objective here is to crop each image into same-size chips, each containing one oil tank, using the bounding box coordinates. \n",
        "\n",
        "For each tank we need to create a binary reference mask. Each mask will created by drawing a disk centered at the x-y midpoint of the bounding box and has a diameter equal to the side of the bounding box. \n",
        "\n",
        "The idea here is: \n",
        "1. to reduce the size of images going into the network without sacrificing spatial resolution, i.e. by re-sizing the input;\n",
        "2. fit multiple chips into the GPU, and thus speed up the training\n",
        "\n",
        "These images will be stored in a new data-set which in turn will need to be seperated into a training and validation subsets.\n",
        "\n",
        "With these done, the last step is to create vector annotations, compatible with the MS COCO format and place them in the appropriate directories.\n",
        "\n",
        "The new data-set directory structure should look like:\n",
        "\n",
        "```\n",
        "/content/\n",
        "  |\n",
        "  |-- newdataset/\n",
        "  |   |-- train/\n",
        "  |   |   |--img001.jpg\n",
        "  |   |   |--img002.jpg\n",
        "  |   |   `--annotations.json\n",
        "  |   |\n",
        "  |   |-- val/\n",
        "  |   |   |--img100.jpg\n",
        "  |   |   |--img101.jpg\n",
        "  |   |   `--annotations.json\n",
        "  |   |\n",
        "  |   `-- masks/\n",
        "  |       |--img001.png\n",
        "  |       |--img002.png\n",
        "  |       |--img100.png\n",
        "  |       `--img101.png\n",
        "  |\n",
        "  `-- Mask_RCNN/\n",
        "      |-- logs/\n",
        "      |   `-- weights.h5\n",
        "      |-- train.py\n",
        "      |-- .gitignore\n",
        "      |-- LICENCE\n",
        "      `-- etc..\n",
        "```\n",
        "\n",
        "The ```newdataset/train``` and ```newdataset/val``` directories store the images selected for training and validation respectively, a ratio of approximately 90% to 10% of the total number of images, and the respective annotation files (json).\n",
        "\n",
        "The ```dataset/masks``` directory contains all mask images of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VxUD4WVXKck"
      },
      "outputs": [],
      "source": [
        "# import some relevant libraries\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import pandas as pd \n",
        "import glob\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UO3UktBYEcW"
      },
      "source": [
        "Read the annotation file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7QsWmh3KO_Vw",
        "outputId": "eceef895-a44b-4dd3-8678-e22878cf8e83"
      },
      "outputs": [],
      "source": [
        "# training dataset\n",
        "anns = pd.read_csv(os.path.join(DATA_DIR, 'annotations.csv'))\n",
        "anns.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDnZJMiEJDR0",
        "outputId": "7e627723-9d2f-43c3-d386-ebfa1474c3af"
      },
      "outputs": [],
      "source": [
        "print(\"There are a total of: \" + str(len(anns)) + \" annotated tank instances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn1bd2DhYJWd"
      },
      "source": [
        "Create a new dataframe that stores the bbox coordinates of each tank for each training image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "3SIPYq9XO_YE",
        "outputId": "6a6a3d7d-a67c-4063-b78a-f3f07103a5a7"
      },
      "outputs": [],
      "source": [
        "# create an empty DataFrame with column names only\n",
        "annotations = pd.DataFrame(columns = ['image_id', 'x', 'y', 'width', 'height', 'class_id'])\n",
        "annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O9sfk-UO_a_"
      },
      "outputs": [],
      "source": [
        "# shape up this dataframe to be compatible with the COCO format;\n",
        "# good for processing the images in their entirety\n",
        "for index, row in anns.iterrows():\n",
        "    fname = anns.iloc[index]['image_id'] + '.jpg'\n",
        "    this_row = anns.iloc[index]['bounds']\n",
        "    res = this_row.replace('(', '')\n",
        "    res = res.replace(')', '')\n",
        "    bbox_list = res.split(\",\")\n",
        "    # extract the bbox attributes of each instance annotation\n",
        "    x = int(bbox_list[0])\n",
        "    y = int(bbox_list[1])\n",
        "    w = int(bbox_list[2]) - int(bbox_list[0]) + 1\n",
        "    h = int(bbox_list[3]) - int(bbox_list[1]) + 1\n",
        "    # append them in the new dataframe\n",
        "    annotations = annotations.append({'image_id' : fname, 'x' : x, 'y' : y, \n",
        "                                      'width' :  w, 'height': h, 'class_id' : 1}, \n",
        "                                     ignore_index = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MEFqr0HPmiU"
      },
      "source": [
        "the above takes approximately 1 min to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dwt_UoyI0sW",
        "outputId": "4e558205-854c-4c60-9408-031327b59431"
      },
      "outputs": [],
      "source": [
        "print(\"There are a total of: \" + str(len(annotations)) + \" annotated tank instances\")\n",
        "print(\"and \" + str(len(annotations.columns)) + \" columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLjd-i_TJs2U",
        "outputId": "aa0248a8-052c-4f85-a895-2cd8a0568e2a"
      },
      "outputs": [],
      "source": [
        "# get the max width and height of tanks from all instances\n",
        "max_width = annotations[\"width\"].max()\n",
        "max_height = annotations[\"height\"].max()\n",
        "max_width, max_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCuNfHQCFbUk"
      },
      "outputs": [],
      "source": [
        "CHIP_SIDE = 128 # chip size that ensures all instances fit within it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co_cI2bCSjwW"
      },
      "outputs": [],
      "source": [
        "# the image parser\n",
        "def get_images(img_dir):\n",
        "    image_set = glob.glob(img_dir+'/'+'*.jpg')\n",
        "    return list(set(image_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS1Ry4eRPnuf"
      },
      "outputs": [],
      "source": [
        "PATH2IMG = '/content/dataset/images/'\n",
        "image_set = get_images(PATH2IMG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oHdstlIWT4s"
      },
      "source": [
        "### Create the chip and mask dataset\n",
        "\n",
        "In this section we will use the dataframe created earlier to:\n",
        "1. cut-out same-size instance chips from the training images;\n",
        "2. create training masks by drawing maximal disks within the bounds of each instance bbox\n",
        "3. create unique names for all chips and store them along with the masks in a new directory "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-mDKOOXr5oy"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/1*p7GqDO8sfSX7kb8R9kX3cQ.png\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlPJzSpOKg1P"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/newdataset/\n",
        "%mkdir /content/newdataset/images/\n",
        "%mkdir /content/newdataset/masks/\n",
        "DATA_DIR = \"/content/newdataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RRUJv1XK3JO"
      },
      "outputs": [],
      "source": [
        "# create an empty DataFrame with column names only\n",
        "new_annotations = pd.DataFrame(columns = ['image_id', 'x', 'y', 'width', 'height', 'class_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfsJcaQTK-nX",
        "outputId": "a44e89bf-6b3e-480d-a647-321e8abb4cdf"
      },
      "outputs": [],
      "source": [
        "PATH2CHIP = '/content/newdataset/images/'\n",
        "PATH2MASK = '/content/newdataset/masks/'\n",
        "\n",
        "for i in range(len(image_set)):\n",
        "  img = cv2.imread(image_set[i])\n",
        "\n",
        "  # create a temp DF that contains all annotations for a given input image\n",
        "  new_df = annotations.loc[PATH2IMG + annotations['image_id'] == image_set[i]]\n",
        "  # reset its index to start from 0\n",
        "  new_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  for index, row in new_df.iterrows():\n",
        "      # get the filename and add the index and .PNG extension to it\n",
        "      this_row = new_df.iloc[index]['image_id']\n",
        "      fname = this_row.split(\".\")[0] + '_' + str(index) + \".png\"\n",
        "      # get the bbox attributes\n",
        "      this_x = new_df.iloc[index]['x']\n",
        "      this_y = new_df.iloc[index]['y']\n",
        "      this_w = new_df.iloc[index]['width']\n",
        "      this_h = new_df.iloc[index]['height']\n",
        "\n",
        "      # check that the attributes are good \n",
        "      if this_w>=1 and this_h>=1:\n",
        "        #  create the chip\n",
        "        blank_image = np.zeros((CHIP_SIDE,CHIP_SIDE,3), np.uint8)\n",
        "        # get the end point x,y- coordinates to define an roi\n",
        "        end_x = this_x + this_w - 1\n",
        "        end_y = this_y + this_h - 1\n",
        "        # the roi contains the desired image patch\n",
        "        roi = img[this_y:end_y, this_x:end_x]\n",
        "        # copy the roi into the top-left part of the blank image\n",
        "        blank_image[:roi.shape[0], :roi.shape[1]]=roi\n",
        "        cv2.imwrite(PATH2CHIP + fname, blank_image)\n",
        "        \n",
        "        # create the mask\n",
        "        blank_image = np.zeros((CHIP_SIDE,CHIP_SIDE,3), np.uint8)\n",
        "        mid_x = int(this_w/2)\n",
        "        mid_y = int(this_h/2)\n",
        "        cv2.circle(blank_image,(mid_x,mid_y), mid_x, (255,255,255), -1)\n",
        "        cv2.imwrite(PATH2MASK + fname, blank_image)\n",
        "\n",
        "        # append the updated info in the new annotation DF\n",
        "        new_annotations = new_annotations.append({'image_id' : fname, 'x' : 0, 'y' : 0, \n",
        "                                          'width' :  this_w, 'height': this_h, 'class_id' : 1}, \n",
        "                                        ignore_index = True)\n",
        "      else:\n",
        "        print('WARNING: skipping chip ' + fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMtt9ceZPw9P"
      },
      "source": [
        "the above takes approximately 1 min to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL_q67aFOEJz",
        "outputId": "cc22a2e2-7aa3-4f66-c4fa-361c6bd816ff"
      },
      "outputs": [],
      "source": [
        "print(len(new_annotations))\n",
        "print(len(new_annotations.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKBaistG4v2R"
      },
      "outputs": [],
      "source": [
        "# do some clean-up\n",
        "#%rm -r /content/dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sDGbB59Xnmp"
      },
      "source": [
        "### Split the images to training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvzKwpCaX9Z_"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/newdataset/train/\n",
        "%mkdir /content/newdataset/val/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnGlTaziXt2q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "# set the path to the images\n",
        "PATH = '/content/newdataset/images/'\n",
        "\n",
        "# get all image filenames\n",
        "list_img=[img for img in os.listdir(PATH) if img.endswith('.png')==True]\n",
        "\n",
        "# convert the list to a dataframe\n",
        "df = pd.DataFrame(list_img)\n",
        "\n",
        "# split \n",
        "data_train, data_val, labels_train, labels_test = train_test_split(df[0], df.index, test_size=0.10, random_state=42)\n",
        "\n",
        "# get the indices of the train and validation DFs\n",
        "train_idx=list(data_train.index)\n",
        "val_idx=list(data_val.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw0FupFgX5An"
      },
      "outputs": [],
      "source": [
        "# absolute path to the training directory \n",
        "TRAIN_PATH = \"/content/newdataset/train/\"\n",
        "\n",
        "# move the training images to TRAIN_PATH\n",
        "for i in range(len(train_idx)):\n",
        "  source = PATH2CHIP + data_train[train_idx[i]]\n",
        "  destination = TRAIN_PATH + data_train[train_idx[i]]\n",
        "  dest = shutil.move(source, destination) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE83vIjJYY1a"
      },
      "outputs": [],
      "source": [
        "# absolute path to the validation directory \n",
        "VAL_PATH = \"/content/newdataset/val/\"\n",
        "\n",
        "# move the training images to VAL_PATH\n",
        "for i in range(len(val_idx)):\n",
        "  source = PATH2CHIP + data_val[val_idx[i]]\n",
        "  destination = VAL_PATH + data_val[val_idx[i]]\n",
        "  dest = shutil.move(source, destination) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOPLZAlr4fIh"
      },
      "outputs": [],
      "source": [
        "# do some clean-up\n",
        "#%rm -r /content/newdataset/images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEDXlrCuWZis"
      },
      "source": [
        "### Annotate the chip masks \n",
        "\n",
        "In this section we will create an MS-COCO compatible annotation file (.JSON) for each of the two sets\n",
        "\n",
        "Let us first review the targeted layout. At a top level, the json file contains the following:\n",
        "```\n",
        "{\n",
        "  \"info\"         : info,\n",
        "  \"licenses\"     : [license],\n",
        "  \"images\"       : [image],\n",
        "  \"annotations\"  : [annotation],\n",
        "  \"categories\"   : [category],\n",
        "}\n",
        "```\n",
        "\n",
        "which extend to the following:\n",
        "```\n",
        "info{\n",
        "  \"year\"          : int, \n",
        "  \"version\"       : str,\n",
        "  \"description\"   : str,\n",
        "  \"contributor\"   : str,\n",
        "  \"url\"           : str,\n",
        "  \"date_captured\" : datetime,\n",
        "}\n",
        "\n",
        "license[{\n",
        "  \"id\"            : int,\n",
        "  \"name\"          : str,\n",
        "  \"url\"           : str,\n",
        "}]\n",
        "\n",
        "image[{\n",
        "  \"id\"            : int,\n",
        "  \"width\"         : int,\n",
        "  \"height\"        : int,\n",
        "  \"file_name\"     : str,\n",
        "  \"license\"       : int,\n",
        "  \"flickr_url\"    : str,\n",
        "  \"coco_url\"      : str,\n",
        "  \"date_captured\" : datetime,\n",
        "}]\n",
        "\n",
        "categories[{\n",
        "  \"id\"            : int,\n",
        "  \"name\"          : str,\n",
        "  \"supercategory\" : str,\n",
        "}]\n",
        "\n",
        "annotation[{\n",
        "  \"id\"            : int,\n",
        "  \"image_id\"      : int,\n",
        "  \"category_id\"   : int,\n",
        "  \"segmentation\"  : RLE or [polygon]\n",
        "  \"area\"          : float,\n",
        "  \"bbox\"          : [x, y, width, height],\n",
        "  \"iscrowd\"       : 0 or 1,\n",
        "}]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19pmbvCJW083"
      },
      "outputs": [],
      "source": [
        "# get the relevant file - annotation code\n",
        "!wget https://raw.githubusercontent.com/georgiosouzounis/instance-segmentation-mask-rcnn/main/annotation/mask2image.py -O /content/mask2image.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVBGLKHHW6am"
      },
      "outputs": [],
      "source": [
        "from mask2image import *\n",
        "import glob\n",
        "\n",
        "# Label ids of the dataset\n",
        "category_ids = {\n",
        "    \"oiltank\": 1\n",
        "}\n",
        "\n",
        "# Define which colors match which categories in the images\n",
        "category_colors = {\n",
        "    \"(255, 255, 255)\": 1 # oil-tank\n",
        "}\n",
        "\n",
        "# Define the ids that are a multiplolygon - none in this case. \n",
        "multipolygon_ids = []\n",
        "\n",
        "# background color\n",
        "background_color_tupple = (0, 0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfHA3Wy0XJeu"
      },
      "outputs": [],
      "source": [
        "# Get \"images\" and \"annotations\" info that need to be placed in the json file\n",
        "def images_annotations_info(maskpath, image_names, image_idx, background_color_tupple):\n",
        "    # This id will be automatically increased as we go\n",
        "    annotation_id = 0\n",
        "    image_id = 0\n",
        "    annotations = []\n",
        "    images = []\n",
        "    \n",
        "    # iterate through all images\n",
        "    for i in range(len(image_idx)):\n",
        "        # get the file name\n",
        "        image_name = image_names[image_idx[i]]\n",
        "        # the mask image is *.png but the original image can be *.jpg.\n",
        "        # we make a reference to the original file in the COCO JSON file\n",
        "        mask_image = os.path.basename(image_name).split(\".\")[0] + \".png\"\n",
        "        # open the mask image\n",
        "        mask_image_open = Image.open(maskpath + mask_image).convert(\"RGB\")\n",
        "        w, h = mask_image_open.size\n",
        "        \n",
        "        # get the \"image\" info block and append it \n",
        "        image = create_image_annotation(image_name, w, h, image_id)\n",
        "        images.append(image)\n",
        "\n",
        "        # get the \"annotations\" info block:\n",
        "        # first create one mask per class (category_colors)\n",
        "        sub_masks = create_sub_masks(mask_image_open, w, h, background_color_tupple)\n",
        "        for color, sub_mask in sub_masks.items():\n",
        "            category_id = category_colors[color]\n",
        "\n",
        "            # create the vector representation of object contours \n",
        "            # and get the \"annotations\" info block\n",
        "            polygons, segmentations = create_sub_mask_annotation(sub_mask)\n",
        "\n",
        "            # Check if we have classes that are a multipolygon\n",
        "            if category_id in multipolygon_ids:\n",
        "                # Combine the polygons to calculate the bounding box and area\n",
        "                multi_poly = MultiPolygon(polygons)\n",
        "                                \n",
        "                annotation = create_annotation_format(multi_poly, segmentations, image_id, category_id, annotation_id)\n",
        "\n",
        "                annotations.append(annotation)\n",
        "                annotation_id += 1\n",
        "            else:\n",
        "                for i in range(len(polygons)):\n",
        "                    # Cleaner to recalculate this variable\n",
        "                    segmentation = [np.array(polygons[i].exterior.coords).ravel().tolist()]\n",
        "                    \n",
        "                    annotation = create_annotation_format(polygons[i], segmentation, image_id, category_id, annotation_id)\n",
        "                    \n",
        "                    annotations.append(annotation)\n",
        "                    annotation_id += 1\n",
        "        image_id += 1\n",
        "    return images, annotations, annotation_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyKLLDRqbGbc"
      },
      "source": [
        "### compute the training set annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTjkfgZEbFRt"
      },
      "outputs": [],
      "source": [
        "# Get the standard COCO JSON format - template\n",
        "coco_format = get_coco_json_format()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFkZWzjrbKei"
      },
      "outputs": [],
      "source": [
        "# create the output file\n",
        "!touch /content/newdataset/train/annotations.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkcPkWmhbOfn",
        "outputId": "985073b5-1289-4122-938a-2f9c8a7b2566"
      },
      "outputs": [],
      "source": [
        "# path to the annotation file\n",
        "ANNOTATION_PATH = \"/content/newdataset/train/annotations.json\"\n",
        "\n",
        "# Create category section\n",
        "coco_format[\"categories\"] = create_category_annotation(category_ids)\n",
        "    \n",
        "# Create images and annotations sections\n",
        "coco_format[\"images\"], coco_format[\"annotations\"], annotation_cnt = images_annotations_info(PATH2MASK, data_train, train_idx, background_color_tupple)\n",
        "\n",
        "with open(ANNOTATION_PATH,\"w\") as ANNOTATION_PATH:\n",
        "  json.dump(coco_format, ANNOTATION_PATH)\n",
        "        \n",
        "print(\"Created %d annotations for training images\" % (annotation_cnt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6yqyFz6dY6I"
      },
      "source": [
        "the above takes approximately 5 min to complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0nZBw_9boez"
      },
      "source": [
        "### compute the validation set annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6bFBxSibntw"
      },
      "outputs": [],
      "source": [
        "# Get the standard COCO JSON format - template\n",
        "coco_format = get_coco_json_format()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dChQohGdbuq4"
      },
      "outputs": [],
      "source": [
        "# create the output file\n",
        "!touch /content/newdataset/val/annotations.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCbL4lI8bx-j",
        "outputId": "1a6360e4-4c0c-4751-b3cc-084078530ca1"
      },
      "outputs": [],
      "source": [
        "# path to the annotation file\n",
        "ANNOTATION_PATH = \"/content/newdataset/val/annotations.json\"\n",
        "        \n",
        "# Create category section\n",
        "coco_format[\"categories\"] = create_category_annotation(category_ids)\n",
        "    \n",
        "# Create images and annotations sections\n",
        "coco_format[\"images\"], coco_format[\"annotations\"], annotation_cnt = images_annotations_info(PATH2MASK, data_val, val_idx, background_color_tupple)\n",
        "\n",
        "with open(ANNOTATION_PATH,\"w\") as ANNOTATION_PATH:\n",
        "  json.dump(coco_format, ANNOTATION_PATH)\n",
        "        \n",
        "print(\"Created %d annotations for validation images\" % (annotation_cnt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYSP7rs8laKz",
        "outputId": "0d5e9a04-01de-4b2d-be5a-237132a71320"
      },
      "outputs": [],
      "source": [
        "# search and report the different file-types in the train image directory\n",
        "!find /content/newdataset/train/ -type f | awk -F. '!a[$NF]++{print $NF}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZx3n97fapK3",
        "outputId": "dde90438-974f-4266-e392-ca563dfd5d3d"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "\n",
        "# simple version for working with CWD\n",
        "name = '/content/newdataset/train/'\n",
        "path, dirs, files = next(os.walk(name))\n",
        "file_count1 = len(files)\n",
        "\n",
        "name = '/content/newdataset/val/'\n",
        "path, dirs, files = next(os.walk(name))\n",
        "file_count2 = len(files)\n",
        "print(file_count1 + file_count2)\n",
        "\n",
        "#expecting 13593 (13591 images + 2 json files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkchZqTJdiUQ"
      },
      "source": [
        "the above takes approximately 50 sec to complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifjP_Kouxa0R"
      },
      "source": [
        "------\n",
        "## Get the code <a name=\"get-the-code\"></a>\n",
        "\n",
        "Lets get a copy of the Matterport Mask R-CNN project modified for TF2 support \n",
        "by Ahmed Gad and further extended by Georgios Ouzounis. It can be found [here](https://github.com/georgiosouzounis/Mask-RCNN-hacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNLcjFe4uzfd"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/georgiosouzounis/Mask-RCNN-hacked.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkCJrjcHR0pa"
      },
      "outputs": [],
      "source": [
        "# Path to trained weights file\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"Mask-RCNN-hacked\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSJgOxZJCIBF"
      },
      "source": [
        "------\n",
        "## Get the COCO weights <a name=\"get-the-coco-weights\"></a>\n",
        "\n",
        "\n",
        "needed for transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9AjpLxRCIP4"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5 -O /content/Mask-RCNN-hacked/mask_rcnn_coco.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1pApBaCRwxZ"
      },
      "outputs": [],
      "source": [
        "# Path to trained weights file\n",
        "COCO_WEIGHTS_PATH = os.path.join(MODEL_DIR, \"mask_rcnn_coco.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9rXei7z3MWg"
      },
      "source": [
        "## Model Training  <a name=\"model-training\"></a>\n",
        "\n",
        "\n",
        "In this section we will configure our data IO (images & masks) driven by the annotation files we created before, create and customize our model, and start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3Pdm49ZKJZk",
        "outputId": "5feb7211-9c96-4d28-e3fc-f4a51f1525da"
      },
      "outputs": [],
      "source": [
        "from imgaug import augmenters as iaa\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'Mask-RCNN-hacked'))  # To find local version of the library\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsdOIz44gvfQ"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as maskUtils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJnra0H3MMJo"
      },
      "outputs": [],
      "source": [
        "############################################################\n",
        "#  Dataset\n",
        "############################################################\n",
        "\n",
        "class OiltankDataset(utils.Dataset):\n",
        "\n",
        "  \"\"\"Load a subset of the custom dataset \n",
        "     + dataset_dir: Root directory of the dataset.\n",
        "     + subset: Subset to load: train or val\n",
        "  \"\"\"\n",
        "  def load_data(self, dataset_dir, subset):\n",
        "    # Train or validation dataset?\n",
        "    assert subset in [\"train\", \"val\"]\n",
        "    # include the subset in the dataset path\n",
        "    dataset_dir = os.path.join(dataset_dir, subset)\n",
        "    # open the relevant annotations file (json) \n",
        "    coco = COCO(\"{}/annotations.json\".format(dataset_dir))\n",
        "    # retrieve the image ids\n",
        "    image_ids = list(coco.imgs.keys())\n",
        "    # add classes; we have only one class to add\n",
        "    self.add_class(\"oiltank\", 1, \"oiltank\")\n",
        "    class_ids = [1]\n",
        "    # add images\n",
        "    for i in image_ids:\n",
        "      self.add_image(\n",
        "        \"oiltank\", # the image source\n",
        "        image_id=i, # the image ID\n",
        "        path=os.path.join(dataset_dir, coco.imgs[i]['file_name']), # the image path\n",
        "        width=CHIP_SIDE, #  image width\n",
        "        height=CHIP_SIDE, # image height\n",
        "        annotations=coco.loadAnns(coco.getAnnIds(imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
        "      \n",
        "  \"\"\"Load instance masks for the given image.\n",
        "     Different datasets use different ways to store masks. This\n",
        "     function converts the different mask formats to one format\n",
        "     in the form of a bitmap [height, width, instances].\n",
        "     \n",
        "     Returns:\n",
        "     + masks: A bool array of shape [height, width, instance count] with\n",
        "              one mask per instance.\n",
        "     + class_ids: a 1D array of class IDs of the instance masks.\n",
        "  \"\"\"    \n",
        "  def load_mask(self, image_id):\n",
        "        \n",
        "        # If not a relevant image, delegate to parent class.\n",
        "        # gives the error: \"You are using the default load_mask(), maybe you need to define your own one.\"\"\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"oiltank\":\n",
        "            print(\"WARNING: not a relevant image\")\n",
        "            return super(OiltankDataset, self).load_mask(image_id)\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        # annotations holds the polygons around each instance  in this image\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            # get the class label\n",
        "            class_id = annotation[\"category_id\"] #self.map_source_class_id(\"coco.{}\".format(annotation['category_id']))\n",
        "\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation, CHIP_SIDE,CHIP_SIDE)\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "                # Is it a crowd? If so, use a negative class ID.\n",
        "                if annotation['iscrowd']:\n",
        "                    # Use negative class ID for crowds\n",
        "                    class_id *= -1\n",
        "                    # For crowd masks, annToMask() sometimes returns a mask\n",
        "                    # smaller than the given dimensions. If so, resize it.\n",
        "                    if m.shape[0] != CHIP_SIDE or m.shape[1] != CHIP_SIDE:\n",
        "                        m = np.ones([CHIP_SIDE, CHIP_SIDE], dtype=bool)\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
        "            class_ids = np.array(class_ids, dtype=np.int32)\n",
        "            return mask, class_ids\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            print(\"WARNING: image does not contain buildings...\")\n",
        "            return super(OiltankDataset, self).load_mask(image_id)\n",
        "\n",
        "  # image_reference simply returns a string that identifies the image \n",
        "  # for debugging purposes. Here it simply returns the path of the image file.\n",
        "  def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"oiltank\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n",
        "\n",
        "  # The following two functions are from pycocotools with a few changes.\n",
        "  # RLE is a simple yet efficient format for storing binary masks. \n",
        "  # RLE first divides a vector (or vectorized image) into a series \n",
        "  # of piecewise constant regions and then for each piece simply \n",
        "  # stores the length of that piece.\n",
        "  def annToRLE(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        segm = ann['segmentation']\n",
        "        if isinstance(segm, list):\n",
        "            # polygon -- a single object might consist of multiple parts\n",
        "            # we merge all parts into one mask rle code\n",
        "            rles = maskUtils.frPyObjects(segm, height, width)\n",
        "            rle = maskUtils.merge(rles)\n",
        "        elif isinstance(segm['counts'], list):\n",
        "            # uncompressed RLE\n",
        "            rle = maskUtils.frPyObjects(segm, height, width)\n",
        "        else:\n",
        "            # rle\n",
        "            rle = ann['segmentation']\n",
        "        return rle\n",
        "\n",
        "  def annToMask(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        rle = self.annToRLE(ann, height, width)\n",
        "        m = maskUtils.decode(rle)\n",
        "        return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs1T5_f4ifZf"
      },
      "outputs": [],
      "source": [
        "PATH2NEWDATA = '/content/newdataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSjN_clap6rl"
      },
      "source": [
        "### Create our MRCNN compatible datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ5WUOXxf8lA",
        "outputId": "e872cdfb-c217-44f9-e2bb-f6544ec5977d"
      },
      "outputs": [],
      "source": [
        "# prepare the training dataset\n",
        "dataset_train = OiltankDataset()\n",
        "dataset_train.load_data(PATH2NEWDATA, \"train\")\n",
        "dataset_train.prepare()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2WkeqREf_L0",
        "outputId": "c1c10632-88cb-448b-ffb6-613525d0086f"
      },
      "outputs": [],
      "source": [
        "# prepare the validation dataset\n",
        "dataset_val = OiltankDataset()\n",
        "dataset_val.load_data(PATH2NEWDATA, \"val\")\n",
        "dataset_val.prepare()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "aVqDjiUEgBgD",
        "outputId": "866b3cc0-f53b-4c3a-a6be-c4b6e31de157"
      },
      "outputs": [],
      "source": [
        "# Load and display a random sample and the bounding boxes\n",
        "class_ids = [0]\n",
        "while class_ids[0] == 0:  ## look for a mask\n",
        "    image_id = random.choice(dataset_train.image_ids)\n",
        "    image_fp = dataset_train.image_reference(image_id)\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "\n",
        "print(image.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "masked = np.zeros(image.shape[:2])\n",
        "for i in range(mask.shape[2]):\n",
        "    masked += image[:, :, 0] * mask[:, :, i]\n",
        "plt.imshow(masked, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "print(image_fp)\n",
        "print(class_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "midpBQL8qFQn"
      },
      "source": [
        "### Customize some image augmentation to reduce the risk of overfitting "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "3xjnPqVpgG8Q",
        "outputId": "cff4c2b0-9827-467a-9a76-0c0374ae2cef"
      },
      "outputs": [],
      "source": [
        "# Image augmentation (light but constant)\n",
        "augmentation = iaa.Sequential([\n",
        "    iaa.OneOf([ ## geometric transform\n",
        "        iaa.Affine(\n",
        "            scale={\"x\": (0.98, 1.02), \"y\": (0.98, 1.04)},\n",
        "            translate_percent={\"x\": (-0.02, 0.02), \"y\": (-0.04, 0.04)},\n",
        "            rotate=(-2, 2),\n",
        "            #shear=(-1, 1),\n",
        "        ),\n",
        "        iaa.PiecewiseAffine(scale=(0.001, 0.025)),\n",
        "    ]),\n",
        "    iaa.OneOf([ ## brightness or contrast\n",
        "        iaa.Multiply((0.9, 1.1)),\n",
        "        iaa.ContrastNormalization((0.9, 1.1)),\n",
        "    ]),#,\n",
        "    #iaa.OneOf([ ## blur or sharpen\n",
        "    #    iaa.GaussianBlur(sigma=(0.0, 0.1)),\n",
        "    #    iaa.Sharpen(alpha=(0.0, 0.1)),\n",
        "    #]),\n",
        "])\n",
        "\n",
        "# test on the same image as above\n",
        "imggrid = augmentation.draw_grid(image[:, :, 0], cols=5, rows=2)\n",
        "plt.figure(figsize=(30, 12))\n",
        "_ = plt.imshow(imggrid[:, :, 0], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpldrQg1L3bH",
        "outputId": "791181c9-b287-47fb-b732-f33d6ac578ad"
      },
      "outputs": [],
      "source": [
        "# The following parameters have been selected to reduce running time for demonstration purposes \n",
        "# These are not optimal!\n",
        "\n",
        "class OiltankConfig(Config):\n",
        "    \"\"\"Configuration for training the oil-tank segmentation model.\n",
        "    Overrides values in the base Config class.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Give the configuration a recognizable name  \n",
        "    NAME = 'oiltank'\n",
        "    \n",
        "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
        "    # GPU because the images are small. Batch size is 12 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 12\n",
        "    \n",
        "    BACKBONE = 'resnet50'\n",
        "    \n",
        "    NUM_CLASSES = 2  # background + 1 oil-tank classes\n",
        "    \n",
        "    # if no resizing is selected:\n",
        "    IMAGE_MIN_DIM = CHIP_SIDE\n",
        "    IMAGE_MAX_DIM = CHIP_SIDE\n",
        "    IMAGE_RESIZE_MODE = 'none'\n",
        "    # alternatively:\n",
        "    #IMAGE_MIN_DIM = 64\n",
        "    #IMAGE_MAX_DIM = 128\n",
        "\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128) # anchor side in pixels\n",
        "\n",
        "    # STEPS_PER_EPOCH should be the number of instances \n",
        "    # divided by (GPU_COUNT*IMAGES_PER_GPU), i.e. 12230/8 = 1020\n",
        "    # and so should VALIDATION_STEPS;  i.e.  1360/8 = 110\n",
        "    # we chose lower number of steps to make our training possible\n",
        "    # in about 4h. However and as a consequence of our choise we will\n",
        "    # see abrupt increases in train and validation loss due to the \n",
        "    # infrequent update of the weights.\n",
        "    STEPS_PER_EPOCH = 200 #it should be set to 1020, this would take over 1h per epoch\n",
        "    VALIDATION_STEPS = 100 #it should be set to 110\n",
        "\n",
        "    # Reduce training ROIs per image because the images are small \n",
        "    # and contain only a few objects at best.\n",
        "    TRAIN_ROIS_PER_IMAGE = 24 \n",
        "    MAX_GT_INSTANCES = 12\n",
        "   \n",
        "    DETECTION_MAX_INSTANCES = 12\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9 # insist on clear detections\n",
        "    DETECTION_NMS_THRESHOLD = 0.5 # background can be misclassified, \n",
        "    # make sure NMS prevents tanks and nearby rings of soil coming together\n",
        "\n",
        "    ## balance out losses\n",
        "    # - rpn_class_loss: How well the Region Proposal Network separates background with objects\n",
        "    # - rpn_bbox_loss : How well the RPN localize objects\n",
        "    # - mrcnn_bbox_loss : How well the Mask RCNN localize objects\n",
        "    # - mrcnn_class_loss : How well the Mask RCNN recognize each class of object\n",
        "    # - mrcnn_mask_loss : How well the Mask RCNN segment objects\n",
        "    LOSS_WEIGHTS = {\n",
        "        \"rpn_class_loss\": 12.0,\n",
        "        \"rpn_bbox_loss\": 1.8,\n",
        "        \"mrcnn_class_loss\": 6.0,\n",
        "        \"mrcnn_bbox_loss\": 1.0,\n",
        "        \"mrcnn_mask_loss\": 1.6\n",
        "    }\n",
        "\n",
        "config = OiltankConfig()\n",
        "config.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QnPhO3bgJVz"
      },
      "source": [
        "Time to train the model! Note that training even a basic model can take a few hours.\n",
        "\n",
        "**Note:** the following model is for demonstration purpose only. We have limited the training to only a few epochs, and have set nominal values for the Oiltank Configuration to reduce run-time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbmB5Im2gMqk"
      },
      "outputs": [],
      "source": [
        "# create the model\n",
        "model = modellib.MaskRCNN(mode='training', config=config, model_dir=MODEL_DIR)\n",
        "\n",
        "# Load the weights. Exclude the last layers because they require a matching \n",
        "# number of classes\n",
        "model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n",
        "    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
        "    \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHbREJWlgQs4"
      },
      "outputs": [],
      "source": [
        "# set the learning rate\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# suppress warnings\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDa-NNR-FrWp"
      },
      "outputs": [],
      "source": [
        "# review the network architecture\n",
        "# the image file is too large. Download it locally and review it\n",
        "model.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBLIV6OtOU_P"
      },
      "outputs": [],
      "source": [
        "# review the network architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crINBJUT2l_J"
      },
      "source": [
        "**INFO:** in the following we will compute a series of training sessions. \n",
        "\n",
        "- In session 1 we will focus on the heads of the model as we are excluding the end layer COCO weights for the purpose of transfer learning. Loss is expected to be a bit bumpy. We will use twice the LR to accelerate computation.\n",
        "- In section 2 we will train the full network to fine tune it to our data. We will use the original LR. If no overfitting occus we expect a smooth and progressive decline of both the training and validation loss.\n",
        "- In section 3, and aiming for bringing the validation loss to about 0.2 we will continue trainiing but with a smaller LR to allow the network to register small changes in weights. \n",
        "- Follow-up sessions can be added if the objective is not met. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xJdXTbRfm3J"
      },
      "source": [
        "**WARNING:** In the following, if after executing it you get an error message:\n",
        "```\n",
        "ResourceExhaustedError:  OOM when allocating tensor with shape[3200,256,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\t [[node mrcnn_mask_deconv_1/conv2d_transpose (defined at /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3009) ]]Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. [Op:__inference_keras_scratch_graph_119840]Function call stack:keras_scratch_graph\n",
        "```\n",
        "go back to your configuration class and set the IMAGES_PER_GPU to a smaller number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3hRND44gNMK",
        "outputId": "eae3f19a-7927-4f38-d4f9-be9bd4b17806"
      },
      "outputs": [],
      "source": [
        "# first we train only the heads, with twice the learning rate for faster execution\n",
        "%%time\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=LEARNING_RATE*2,\n",
        "            epochs=6,\n",
        "            layers='heads',\n",
        "            augmentation=augmentation)\n",
        "\n",
        "# if this is your first run:\n",
        "history = model.keras_model.history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDkDlZBaTHUn",
        "outputId": "a3967377-4c13-4d31-d311-6d70cfbfd3b0"
      },
      "outputs": [],
      "source": [
        "# then, all layers are trained to fine-tune the model to our data, for another 6 epochs.\n",
        "%%time\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            epochs=12,\n",
        "            layers='all',\n",
        "            augmentation=augmentation)\n",
        "\n",
        "new_history = model.keras_model.history.history\n",
        "for k in new_history: history[k] = history[k] + new_history[k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ek9NhrhTe-b",
        "outputId": "31b35b04-d7be-4c53-8c84-3865dab38bdc"
      },
      "outputs": [],
      "source": [
        "# lastly we train all layers at a reduced LR for another 12 epochs\n",
        "%%time\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=LEARNING_RATE/5,\n",
        "            epochs=24,\n",
        "            layers='all',\n",
        "            augmentation=augmentation)\n",
        "\n",
        "new_history = model.keras_model.history.history\n",
        "for k in new_history: history[k] = history[k] + new_history[k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHqo7Ubg2-v7"
      },
      "source": [
        "Model training requires approximately 55min + 60min + 125min = 4h approximately to complete "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuAy55LLT9H6"
      },
      "source": [
        "### A brief look into the history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfdh6j68ga4Q"
      },
      "outputs": [],
      "source": [
        "# get the model training stats\n",
        "epochs = range(1,len(next(iter(history.values())))+1)\n",
        "pd.DataFrame(history, index=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "BFaKYpj5geJz",
        "outputId": "8cd96890-d9bf-4c87-bffc-6a8285baae03"
      },
      "outputs": [],
      "source": [
        "# plot the training vs validation loss, and others \n",
        "plt.figure(figsize=(17,5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.plot(epochs, history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIF2sNhVUCDN"
      },
      "source": [
        "From the plot above one can see a smooth reduction of training loss which is a good indication regarding the model's complexity. I.e. no evident signs of overfitting can be seen. Flactuations in the validation loss can be due to many reasons. Overall and if one was to fit a polynomial, between the edge points of the orange line, the result would be a curve following the trends of the blue line. \n",
        "\n",
        "Note that increasing the number of epochs by running an additional training session may not help much as the model seems to be saturated, i.e. it cannot be improved further. It this case, re-adjustment of the hyperparameters is needed followed by a repetition of the training sessions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E85aoZpOr9lt"
      },
      "source": [
        "### Select the best weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JqJqoa5ggUJ",
        "outputId": "5026d80e-2e7a-4f76-f970-56dc370fe5cd"
      },
      "outputs": [],
      "source": [
        "# select the best weights based on the lowest validation loss\n",
        "best_epoch = np.argmin(history[\"val_loss\"])\n",
        "print(\"Best Epoch:\", best_epoch + 1, history[\"val_loss\"][best_epoch])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpabBrxQgjCT",
        "outputId": "4f51893c-d904-47c5-db84-509fade03a5b"
      },
      "outputs": [],
      "source": [
        "# select best model \n",
        "LOG_DIR = \"/content/Mask-RCNN-hacked/log/\"\n",
        "dir_names = next(os.walk(LOG_DIR))[1]\n",
        "key = config.NAME.lower()\n",
        "dir_names = filter(lambda f: f.startswith(key), dir_names)\n",
        "dir_names = sorted(dir_names)\n",
        "\n",
        "if not dir_names:\n",
        "    import errno\n",
        "    raise FileNotFoundError(\n",
        "        errno.ENOENT,\n",
        "        \"Could not find model directory under {}\".format(self.model_dir))\n",
        "    \n",
        "fps = []\n",
        "# Pick last directory\n",
        "for d in dir_names: \n",
        "    dir_name = os.path.join(LOG_DIR, d)\n",
        "    # Find the last checkpoint\n",
        "    checkpoints = next(os.walk(dir_name))[2]\n",
        "    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n",
        "    checkpoints = sorted(checkpoints)\n",
        "    if not checkpoints:\n",
        "        print('No weight files in {}'.format(dir_name))\n",
        "    else:\n",
        "        checkpoint = os.path.join(dir_name, checkpoints[best_epoch])\n",
        "        fps.append(checkpoint)\n",
        "\n",
        "model_path = sorted(fps)[-1]\n",
        "print('Found model {}'.format(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPpUXcKLglJD"
      },
      "source": [
        "## Model Inference <a name=\"model-inference\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Av6Ie613xl"
      },
      "source": [
        "### Get the best model/weights\n",
        "\n",
        "If stored externally, modify the path below to bring up a copy of your best  weights file into this project. Otherwise **skip** the next 2 code blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7hYg0JZ2JKP"
      },
      "outputs": [],
      "source": [
        "%cp /content/drive/MyDrive/deep-learning/mask-rcnn/logs/mask_rcnn_oiltank_0021.h5 /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUWOUsfd2kUK"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/mask_rcnn_oiltank_0021.h5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5N2Y9_gf7Io"
      },
      "source": [
        "### Create the inference model configured with the best weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbmQbMPOgm37",
        "outputId": "fefe9a1f-c172-443e-cc55-0a0536e63586"
      },
      "outputs": [],
      "source": [
        "class InferenceConfig(OiltankConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode='inference', \n",
        "                          config=inference_config,\n",
        "                          model_dir=ROOT_DIR)\n",
        "\n",
        "# Load trained weights (fill in path to trained weights here)\n",
        "assert model_path != \"\", \"Provide path to trained weights\"\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5GUfBUQf-wI"
      },
      "source": [
        "### Demo a random set of GT vs predictions from images in the Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqV9b78OgpZp"
      },
      "outputs": [],
      "source": [
        "# set the color for the airplanes class\n",
        "def get_colors_for_class_ids(class_ids):\n",
        "    colors = []\n",
        "    for class_id in class_ids:\n",
        "        if class_id == 1:\n",
        "            colors.append((.941, .204, .204))\n",
        "    return colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W5jLwNSpgqFH",
        "outputId": "9677c041-038e-4927-c823-ca2274862c8c"
      },
      "outputs": [],
      "source": [
        "# Show few example of ground truth vs. predictions on the validation dataset \n",
        "dataset = dataset_val\n",
        "fig = plt.figure(figsize=(10, 30))\n",
        "\n",
        "for i in range(6):\n",
        "\n",
        "    image_id = random.choice(dataset.image_ids)\n",
        "    \n",
        "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config, \n",
        "                               image_id, use_mini_mask=False)\n",
        "    \n",
        "    print(original_image.shape)\n",
        "    plt.subplot(6, 2, 2*i + 1)\n",
        "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                                dataset.class_names,\n",
        "                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n",
        "    \n",
        "    plt.subplot(6, 2, 2*i + 2)\n",
        "    results = model.detect([original_image]) #, verbose=1)\n",
        "    r = results[0]\n",
        "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                                dataset.class_names, r['scores'], \n",
        "                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPddiusZgQkp"
      },
      "source": [
        "### Experiment with a small test set of previously unseen chips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DALdJwPK4LUO"
      },
      "source": [
        "get the test_chip set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3D2jJ9a1QHu"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/georgiosouzounis/instance-segmentation-mask-rcnn/raw/main/data/test_chips.zip -O /content/test_chips.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxKhbiQH24ap",
        "outputId": "676eabc8-c14a-4ded-daeb-7c2f00dcfd04"
      },
      "outputs": [],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN3vRkfF2dP2"
      },
      "outputs": [],
      "source": [
        "!unzip /content/test_chips.zip \n",
        "%rm /content/test_chips.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0-6tJPqXlzh"
      },
      "outputs": [],
      "source": [
        "test_dir = \"/content/test_chips/\"\n",
        "\n",
        "# Get the test image filenames\n",
        "test_image_set = glob.glob(test_dir+'/'+'*.png')\n",
        "test_image_set = list(set(test_image_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5luXSXQYC_5"
      },
      "outputs": [],
      "source": [
        "CLASS_NAMES = [\"background\", \"oiltank\"]\n",
        "RES_DIR = \"/content/results/\"\n",
        "import mrcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP_yR6jGZUSL"
      },
      "outputs": [],
      "source": [
        "def vis_and_save(image_id, _min_score):\n",
        "  image = cv2.imread(image_id)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  results = model.detect([image]) #, verbose=1)\n",
        "  r = results[0]\n",
        "\n",
        "  basename = os.path.basename(image_id).split(\".\")[0]\n",
        "  print(\"displaying chip: \" + basename)\n",
        "\n",
        "  mrcnn.visualize.display_instances(image=image, \n",
        "                                  boxes=r['rois'], \n",
        "                                  masks=r['masks'], \n",
        "                                  class_ids=r['class_ids'], \n",
        "                                  class_names=CLASS_NAMES, \n",
        "                                  scores=r['scores'],min_score=_min_score)\n",
        "  outfname =  RES_DIR + basename + \"_seg.png\"\n",
        "  cv2.imwrite(outfname, image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9QjiBhraX5s4",
        "outputId": "49e0aa84-5c1d-4df8-b97b-36afffbdd959"
      },
      "outputs": [],
      "source": [
        "# set the min score threshold\n",
        "_min_score = 0.99\n",
        "\n",
        "# display the results for all chips\n",
        "for image_id in tqdm(test_image_set):\n",
        "  vis_and_save(image_id, _min_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cURjPfuFdlBH"
      },
      "source": [
        "## Conclusions <a name=\"conclusions\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlzXFaGHdqbv"
      },
      "source": [
        "Reviewing the results shows that the model performs rather well but in some cases has a hard time seperating accurately the the oil-tank perimeter from the tank itself. This is referred to as leakage and is primarily due to the very little information on the background. Recall that all chips are focused on the tank with some minor background areas seen at the corners of the BBox and the rest of the chip content is black. This calls for a better annotation strategy.\n",
        "\n",
        "Regarding the successful detections, they apper to be proper though it is evident that some additional improvement in the model (loss reduction) would have been appreciated.\n",
        "\n",
        "Lastly, we see one False Negative and that is on a type of tank that is not represented sufficiently in our dataset. More training instances of this kind would help.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHG8GzRfdrO0"
      },
      "source": [
        "### Examples of 'leakage'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "DUp7C3oYdolU",
        "outputId": "be079fca-6ef1-4efb-8cf8-993f7b708606"
      },
      "outputs": [],
      "source": [
        "vis_and_save(\"/content/test_chips/test6.png\", 0.94)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh3P-ABTd0c7"
      },
      "source": [
        "### Examples of 'False Negatives' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "FpyLHTGPdxtt",
        "outputId": "086337c8-2a55-4a9f-b85d-33c446b2a693"
      },
      "outputs": [],
      "source": [
        "vis_and_save(\"/content/test_chips/test9.png\", 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8UfAcBZ6Q4U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mask-rcnn-oiltanks-gpu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
